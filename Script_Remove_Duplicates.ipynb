{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uG-P04hF4Ll",
        "outputId": "cf859187-04f5-42ad-ed03-7f7340eb9282"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this script, modify `file_path` to your specific file path. I have the `\"/content/` placeholder for use w/in a Google Colab runtime file for simple and quick use. The script also exports to this destination by default.\n",
        "\n",
        "1. Identify duplicates in the file - I'm working off a single column \"Names\" - for my use case I wanted to know what the dups were, not just remove them from the list.\n",
        "\n",
        "2. Remove the duplicates and create a \"clean\" file.\n",
        "\n",
        "3. Write new files to my file destination - one that has the clean .csv file and the other that lists the names that were duplicated.\n",
        "\n",
        "Note: I'm using `keep='first'` b/c I *don't* want to totally remove the duplicates, I just want to remove all of the identical names after I have found one instance of it.\n",
        "\n",
        "Pandas documentation on the `drop_duplicated` method here: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop_duplicates.html"
      ],
      "metadata": {
        "id": "i0D9sKxihIkO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6LLiZk6Ft9k",
        "outputId": "b1892ef0-0e9b-4f07-f374-d1966446da70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete!\n",
            "Cleaned data saved to 'Cleaned_Data.csv'.\n",
            "Log of removed duplicates saved to 'Duplicates_Log.csv'.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = \"/content/Monday.com CSV Consolidate - WORKING.csv\"  # Replace with the path to your CSV file\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Identify duplicate names\n",
        "duplicates = data[data.duplicated(subset=\"Name\", keep='first')]\n",
        "\n",
        "# Remove duplicate rows\n",
        "cleaned_data = data.drop_duplicates(subset=\"Name\", keep='first')\n",
        "\n",
        "# Save the cleaned data and the log\n",
        "cleaned_data.to_csv(\"Cleaned_Data.csv\", index=False)\n",
        "duplicates.to_csv(\"Duplicates_Log.csv\", index=False)\n",
        "\n",
        "print(\"Processing complete!\")\n",
        "print(\"Cleaned data saved to 'Cleaned_Data.csv'.\")\n",
        "print(\"Log of removed duplicates saved to 'Duplicates_Log.csv'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t7dW0gdOF-mS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}